{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fce271",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a2ef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b210dc2c9aa0426f98484e4a168c4802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "ds = load_dataset(\"heegyu/news-category-dataset\")\n",
    "\n",
    "def join_cols(batch):\n",
    "    head = batch.get(\"headline\", \"\") or \"\"\n",
    "    desc = batch.get(\"short_description\", \"\") or \"\"\n",
    "    batch[\"text\"] = (head + \" \" + desc).strip()\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(join_cols)\n",
    "\n",
    "label_names = sorted(list(set(ds[\"train\"][\"category\"])))\n",
    "label2id = {lab: i for i, lab in enumerate(label_names)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "def encode_label(batch):\n",
    "    batch[\"labels\"] = label2id[batch[\"category\"]]\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(encode_label)\n",
    "ds = ds.cast_column(\"labels\", ClassLabel(names=label_names))\n",
    "\n",
    "tmp = ds[\"train\"].train_test_split(test_size=0.2, seed=42, stratify_by_column=\"labels\")\n",
    "valtest = tmp[\"test\"].train_test_split(test_size=0.5, seed=42, stratify_by_column=\"labels\")\n",
    "\n",
    "train_subset = tmp[\"train\"].select(range(20000))\n",
    "val_subset = valtest[\"train\"].select(range(2000))\n",
    "test_subset = valtest[\"test\"].select(range(2000))\n",
    "\n",
    "full_train_subset = tmp[\"train\"]\n",
    "full_val_subset = valtest[\"train\"]\n",
    "full_test_subset = valtest[\"test\"]\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "def tokenize_split(dset):\n",
    "    keep = {\"text\", \"labels\"}\n",
    "    remove_cols = [c for c in dset.column_names if c not in keep]\n",
    "    return dset.map(tok_fn, batched=True, remove_columns=remove_cols)\n",
    "\n",
    "tok_train_subset = tokenize_split(train_subset)\n",
    "tok_val_subset = tokenize_split(val_subset)\n",
    "tok_test_subset = tokenize_split(test_subset)\n",
    "tok_full_train = tokenize_split(full_train_subset)\n",
    "tok_full_val = tokenize_split(full_val_subset)\n",
    "tok_full_test = tokenize_split(full_test_subset)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_micro\": f1.compute(predictions=preds, references=labels, average=\"micro\")[\"f1\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a283877",
   "metadata": {},
   "source": [
    "# Full Fine-tuning with DistilBERT_base_uncased using full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcdb576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_27296\\1840196798.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10477' max='10477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10477/10477 21:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.140900</td>\n",
       "      <td>1.071866</td>\n",
       "      <td>0.687921</td>\n",
       "      <td>0.687921</td>\n",
       "      <td>0.569490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='655' max='655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [655/655 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 1.0572384595870972, 'eval_accuracy': 0.6932181549181502, 'eval_f1_micro': 0.6932181549181502, 'eval_f1_macro': 0.5674181930441067, 'eval_runtime': 46.2001, 'eval_samples_per_second': 453.528, 'eval_steps_per_second': 14.177, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('huffpost-generic-distilbert-FULL\\\\tokenizer_config.json',\n",
       " 'huffpost-generic-distilbert-FULL\\\\special_tokens_map.json',\n",
       " 'huffpost-generic-distilbert-FULL\\\\vocab.txt',\n",
       " 'huffpost-generic-distilbert-FULL\\\\added_tokens.json',\n",
       " 'huffpost-generic-distilbert-FULL\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model (generic, no prior news fine-tuning)\n",
    "num_labels = len(label_names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Training config\n",
    "run_name = \"huffpost-generic-distilbert-FULL\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=run_name,\n",
    "    run_name=run_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tok_full_train,\n",
    "    eval_dataset=tok_full_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics = trainer.evaluate(tok_full_test)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "# Save model\n",
    "model.save_pretrained(run_name)\n",
    "tok.save_pretrained(run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cc0b5",
   "metadata": {},
   "source": [
    "# Prompt Tuning 100 tokens with DistilBERT_base_uncased using full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd8040bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Prompt Tuning (100 tokens, FULL data) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_27296\\3506899490.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 699,690 || all params: 67,685,460 || trainable%: 1.0337\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10477' max='10477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10477/10477 29:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.658800</td>\n",
       "      <td>2.542844</td>\n",
       "      <td>0.362001</td>\n",
       "      <td>0.362001</td>\n",
       "      <td>0.076463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='655' max='655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [655/655 01:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tuning (100 tokens, FULL data) test metrics: {'eval_loss': 2.5396175384521484, 'eval_accuracy': 0.3636710733546509, 'eval_f1_micro': 0.3636710733546509, 'eval_f1_macro': 0.0767062106376314, 'eval_runtime': 95.4866, 'eval_samples_per_second': 219.434, 'eval_steps_per_second': 6.86, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('huffpost-generic-distilbert-prompt100-FULL\\\\tokenizer_config.json',\n",
       " 'huffpost-generic-distilbert-prompt100-FULL\\\\special_tokens_map.json',\n",
       " 'huffpost-generic-distilbert-prompt100-FULL\\\\vocab.txt',\n",
       " 'huffpost-generic-distilbert-prompt100-FULL\\\\added_tokens.json',\n",
       " 'huffpost-generic-distilbert-prompt100-FULL\\\\tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PromptTuningConfig, get_peft_model\n",
    "\n",
    "num_labels = len(label_names)\n",
    "num_layers = 6\n",
    "token_dim = 768\n",
    "\n",
    "print(\"\\n=== Training Prompt Tuning (100 tokens, FULL data) ===\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "prompt_cfg = PromptTuningConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    num_virtual_tokens=100,\n",
    "    tokenizer_name_or_path=checkpoint,\n",
    "    num_layers=num_layers,\n",
    "    token_dim=token_dim,\n",
    "    num_attention_heads=12,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, prompt_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "run_name = \"huffpost-generic-distilbert-prompt100-FULL\"\n",
    "args = TrainingArguments(\n",
    "    output_dir=run_name,\n",
    "    run_name=run_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tok_full_train,\n",
    "    eval_dataset=tok_full_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_metrics = trainer.evaluate(tok_full_test)\n",
    "print(\"Prompt tuning (100 tokens, FULL data) test metrics:\", test_metrics)\n",
    "\n",
    "model.save_pretrained(run_name)\n",
    "tok.save_pretrained(run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1910c",
   "metadata": {},
   "source": [
    "# Using another base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d07b4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, IA3Config, PromptTuningConfig, get_peft_model\n",
    "\n",
    "# Common\n",
    "base_model = \"huffpost-generic-distilbert-FULL\"\n",
    "num_labels = len(label_names)\n",
    "\n",
    "def make_trainer(model):\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"tmp\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=200,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    collator = DataCollatorWithPadding(tok)\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator,\n",
    "        train_dataset=tok_train_subset,\n",
    "        eval_dataset=tok_val_subset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af776834",
   "metadata": {},
   "source": [
    "## LoRA rank=2 using another base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f9f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_27296\\3932303399.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>1.049925</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.536678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA test: {'eval_loss': 1.0978201627731323, 'eval_accuracy': 0.6795, 'eval_f1_micro': 0.6795, 'eval_f1_macro': 0.548948551407006, 'eval_runtime': 5.1008, 'eval_samples_per_second': 392.095, 'eval_steps_per_second': 12.351, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# LoRA\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "trainer = make_trainer(model)\n",
    "trainer.train()\n",
    "print(\"LoRA test:\", trainer.evaluate(tok_test_subset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7dacbb",
   "metadata": {},
   "source": [
    "## IA3 small using another base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7917ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_27296\\3932303399.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>1.049775</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.536798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IA3 test: {'eval_loss': 1.0973131656646729, 'eval_accuracy': 0.681, 'eval_f1_micro': 0.681, 'eval_f1_macro': 0.5497769557756874, 'eval_runtime': 4.6748, 'eval_samples_per_second': 427.829, 'eval_steps_per_second': 13.477, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# IA3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "ia3_cfg = IA3Config(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    feedforward_modules=[],\n",
    ")\n",
    "model = get_peft_model(model, ia3_cfg)\n",
    "trainer = make_trainer(model)\n",
    "trainer.train()\n",
    "print(\"IA3 test:\", trainer.evaluate(tok_test_subset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70786b43",
   "metadata": {},
   "source": [
    "## Prompt Tuning 10 tokens with another base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c198ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_27296\\3932303399.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.182200</td>\n",
       "      <td>1.163325</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.500321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tuning test: {'eval_loss': 1.2199472188949585, 'eval_accuracy': 0.6455, 'eval_f1_micro': 0.6455, 'eval_f1_macro': 0.5043429011990411, 'eval_runtime': 5.2205, 'eval_samples_per_second': 383.103, 'eval_steps_per_second': 12.068, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Prompt Tuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "prompt_cfg = PromptTuningConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    num_virtual_tokens=10,\n",
    "    tokenizer_name_or_path=base_model,\n",
    "    num_layers=6,\n",
    "    token_dim=768,\n",
    "    num_attention_heads=12,\n",
    "\n",
    ")\n",
    "model = get_peft_model(model, prompt_cfg)\n",
    "trainer = make_trainer(model)\n",
    "trainer.train()\n",
    "print(\"Prompt Tuning test:\", trainer.evaluate(tok_test_subset))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
