{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0de277",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7175e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': Value('string'), 'headline': Value('string'), 'category': Value('string'), 'short_description': Value('string'), 'authors': Value('string'), 'date': Value('timestamp[s]'), 'text': Value('string'), 'labels': ClassLabel(names=['ARTS', 'ARTS & CULTURE', 'BLACK VOICES', 'BUSINESS', 'COLLEGE', 'COMEDY', 'CRIME', 'CULTURE & ARTS', 'DIVORCE', 'EDUCATION', 'ENTERTAINMENT', 'ENVIRONMENT', 'FIFTY', 'FOOD & DRINK', 'GOOD NEWS', 'GREEN', 'HEALTHY LIVING', 'HOME & LIVING', 'IMPACT', 'LATINO VOICES', 'MEDIA', 'MONEY', 'PARENTING', 'PARENTS', 'POLITICS', 'QUEER VOICES', 'RELIGION', 'SCIENCE', 'SPORTS', 'STYLE', 'STYLE & BEAUTY', 'TASTE', 'TECH', 'THE WORLDPOST', 'TRAVEL', 'U.S. NEWS', 'WEDDINGS', 'WEIRD NEWS', 'WELLNESS', 'WOMEN', 'WORLD NEWS', 'WORLDPOST'])}\n",
      "{'train': Dataset({\n",
      "    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'text', 'labels'],\n",
      "    num_rows: 20000\n",
      "}), 'validation': Dataset({\n",
      "    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'text', 'labels'],\n",
      "    num_rows: 2000\n",
      "}), 'test': Dataset({\n",
      "    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'text', 'labels'],\n",
      "    num_rows: 2000\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "# Requirements:\n",
    "#   pip install datasets transformers accelerate evaluate scikit-learn\n",
    "\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset (HuffPost / News Category)\n",
    "ds = load_dataset(\"heegyu/news-category-dataset\") \n",
    "\n",
    "# Concatenate headline + short_description -> text\n",
    "def join_cols(batch):\n",
    "    head = batch.get(\"headline\", \"\") or \"\"\n",
    "    desc = batch.get(\"short_description\", \"\") or \"\"\n",
    "    batch[\"text\"] = (head + \" \" + desc).strip()\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(join_cols)\n",
    "\n",
    "# Build label mapping (string categories -> ids)\n",
    "label_names = sorted(list(set(ds[\"train\"][\"category\"])))\n",
    "label2id = {lab: i for i, lab in enumerate(label_names)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "def encode_label(batch):\n",
    "    batch[\"labels\"] = label2id[batch[\"category\"]]\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(encode_label)\n",
    "\n",
    "# cast AFTER labels exist\n",
    "from datasets import ClassLabel\n",
    "labels_cls = ClassLabel(names=label_names)\n",
    "ds = ds.cast_column(\"labels\", labels_cls)\n",
    "\n",
    "print(ds[\"train\"].features)  # sanity check: 'labels' should be ClassLabel\n",
    "\n",
    "# 4) Stratified train/val/test split (80/10/10) by 'labels'\n",
    "tmp = ds[\"train\"].train_test_split(test_size=0.2, seed=42, stratify_by_column=\"labels\")\n",
    "valtest = tmp[\"test\"].train_test_split(test_size=0.5, seed=42, stratify_by_column=\"labels\")\n",
    "\n",
    "train_subset = tmp[\"train\"].select(range(20000))      \n",
    "val_subset   = valtest[\"train\"].select(range(2000))  \n",
    "test_subset  = valtest[\"test\"].select(range(2000))\n",
    "\n",
    "ds_splits = {\"train\": train_subset, \"validation\": val_subset, \"test\": test_subset}\n",
    "\n",
    "print(ds_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc89d4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98369b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc3c70f870b4d77a641b4385eae60e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer (DistilBERT base; generic)\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "cols_to_remove = list(set(ds_splits[\"train\"].column_names) - set([\"text\", \"labels\"]))\n",
    "ds_tok = {k: v.map(tok_fn, batched=True, remove_columns=cols_to_remove) for k, v in ds_splits.items()}\n",
    "\n",
    "# 6) Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    out = {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_micro\": f1.compute(predictions=preds, references=labels, average=\"micro\")[\"f1\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9db39d",
   "metadata": {},
   "source": [
    "# Train Full Fine-tuneing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a593a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\1336083041.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 02:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.801000</td>\n",
       "      <td>1.737012</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.247974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=2.173415771484375, metrics={'train_runtime': 149.6471, 'train_samples_per_second': 133.648, 'train_steps_per_second': 8.353, 'total_flos': 391794960991104.0, 'train_loss': 2.173415771484375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model (generic, no prior news fine-tuning)\n",
    "num_labels = len(label_names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Training config\n",
    "run_name = \"huffpost-generic-distilbert\"\n",
    "args = TrainingArguments(\n",
    "    output_dir=run_name,\n",
    "    run_name=run_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,        \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",                \n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0c3abc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 1.7709825038909912, 'eval_accuracy': 0.555, 'eval_f1_micro': 0.555, 'eval_f1_macro': 0.2527760201367594, 'eval_runtime': 4.7029, 'eval_samples_per_second': 425.266, 'eval_steps_per_second': 13.396, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test\n",
    "test_metrics = trainer.evaluate(ds_tok[\"test\"])\n",
    "print(\"Test metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d390759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'SCIENCE', 'score': 0.10476445406675339}]\n",
      "[{'label': 'BUSINESS', 'score': 0.2333836406469345}]\n"
     ]
    }
   ],
   "source": [
    "# Save model, tokenizer, and a simple metrics file\n",
    "trainer.save_model(run_name)\n",
    "tok.save_pretrained(run_name)\n",
    "\n",
    "os.makedirs(run_name, exist_ok=True)\n",
    "with open(os.path.join(run_name, \"metrics.txt\"), \"w\") as f:\n",
    "    f.write(\"\\nTest:\\n\")\n",
    "    for k, v in test_metrics.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "clf = pipeline(\"text-classification\", model=run_name, tokenizer=tok, truncation=True)\n",
    "print(clf(\"NASA announces new Artemis mission milestone\"))\n",
    "print(clf(\"The stock market crashed today due to economic uncertainty.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b484feb",
   "metadata": {},
   "source": [
    "# Train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0c47197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LoRA rank 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\595169320.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 696,618 || all params: 67,682,388 || trainable%: 1.0292\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.594300</td>\n",
       "      <td>2.559246</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>0.078764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 2: {'eval_loss': 2.562145233154297, 'eval_accuracy': 0.382, 'eval_f1_micro': 0.382, 'eval_f1_macro': 0.08178060379841959, 'eval_runtime': 5.1587, 'eval_samples_per_second': 387.698, 'eval_steps_per_second': 12.212, 'epoch': 1.0}\n",
      "\n",
      "=== Training LoRA rank 8 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\595169320.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 917,802 || all params: 67,903,572 || trainable%: 1.3516\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.573800</td>\n",
       "      <td>2.538581</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.076739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 8: {'eval_loss': 2.5357024669647217, 'eval_accuracy': 0.3795, 'eval_f1_micro': 0.3795, 'eval_f1_macro': 0.0790130870054391, 'eval_runtime': 5.1401, 'eval_samples_per_second': 389.099, 'eval_steps_per_second': 12.257, 'epoch': 1.0}\n",
      "\n",
      "=== Training LoRA rank 16 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\595169320.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,212,714 || all params: 68,198,484 || trainable%: 1.7782\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.576600</td>\n",
       "      <td>2.542004</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.075045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 16: {'eval_loss': 2.53794002532959, 'eval_accuracy': 0.3805, 'eval_f1_micro': 0.3805, 'eval_f1_macro': 0.07921883093632665, 'eval_runtime': 5.0554, 'eval_samples_per_second': 395.617, 'eval_steps_per_second': 12.462, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "for rank in [2, 8, 16]:\n",
    "    print(f\"\\n=== Training LoRA rank {rank} ===\")\n",
    "\n",
    "    # Base model\n",
    "    num_labels = len(label_names)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # LoRA config\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Training config\n",
    "    run_name = f\"huffpost-generic-distilbert-lora-r{rank}\"\n",
    "    args = TrainingArguments(\n",
    "        output_dir=run_name,\n",
    "        run_name=run_name,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=200,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator,\n",
    "        train_dataset=ds_tok[\"train\"],\n",
    "        eval_dataset=ds_tok[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate\n",
    "    test_metrics = trainer.evaluate(ds_tok[\"test\"])\n",
    "    print(f\"rank {rank}:\", test_metrics)\n",
    "\n",
    "    # Save model, tokenizer\n",
    "    model.save_pretrained(run_name)\n",
    "    tokenizer.save_pretrained(run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9cb3b",
   "metadata": {},
   "source": [
    "## Train IA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36403c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training IA3 (small) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\3690024637.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 632,106 || all params: 67,617,876 || trainable%: 0.9348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.042400</td>\n",
       "      <td>2.994007</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.026468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IA3-small: {'eval_loss': 3.012336015701294, 'eval_accuracy': 0.2635, 'eval_f1_micro': 0.2635, 'eval_f1_macro': 0.027034833403383144, 'eval_runtime': 4.5488, 'eval_samples_per_second': 439.676, 'eval_steps_per_second': 13.85, 'epoch': 1.0}\n",
      "\n",
      "=== Training IA3 (medium) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\3690024637.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 636,714 || all params: 67,622,484 || trainable%: 0.9416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.026600</td>\n",
       "      <td>2.974749</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.029411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IA3-medium: {'eval_loss': 2.9899487495422363, 'eval_accuracy': 0.276, 'eval_f1_micro': 0.276, 'eval_f1_macro': 0.028922916254079084, 'eval_runtime': 4.6643, 'eval_samples_per_second': 428.785, 'eval_steps_per_second': 13.507, 'epoch': 1.0}\n",
      "\n",
      "=== Training IA3 (full) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\3690024637.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 659,754 || all params: 67,645,524 || trainable%: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.994400</td>\n",
       "      <td>2.941963</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.030917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IA3-full: {'eval_loss': 2.9561524391174316, 'eval_accuracy': 0.283, 'eval_f1_micro': 0.283, 'eval_f1_macro': 0.02897736088359369, 'eval_runtime': 7.2178, 'eval_samples_per_second': 277.092, 'eval_steps_per_second': 8.728, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from peft import IA3Config, get_peft_model\n",
    "\n",
    "ia3_variants = {\n",
    "    \"small\":  [\"q_lin\", \"v_lin\"],\n",
    "    \"medium\": [\"q_lin\", \"v_lin\", \"out_lin\"],\n",
    "    \"full\":   [\"q_lin\", \"v_lin\", \"out_lin\", \"lin1\", \"lin2\"],\n",
    "}\n",
    "\n",
    "for name, target_mods in ia3_variants.items():\n",
    "    print(f\"\\n=== Training IA3 ({name}) ===\")\n",
    "\n",
    "    num_labels = len(label_names)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # ✅ only include FFN modules if they’re inside target_mods\n",
    "    feedforward_mods = [m for m in [\"lin1\", \"lin2\"] if m in target_mods]\n",
    "\n",
    "    ia3_cfg = IA3Config(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=target_mods,\n",
    "        feedforward_modules=feedforward_mods,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, ia3_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    run_name = f\"huffpost-generic-distilbert-ia3-{name}\"\n",
    "    args = TrainingArguments(\n",
    "        output_dir=run_name,\n",
    "        run_name=run_name,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=200,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator,\n",
    "        train_dataset=ds_tok[\"train\"],\n",
    "        eval_dataset=ds_tok[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    test_metrics = trainer.evaluate(ds_tok[\"test\"])\n",
    "    print(f\"IA3-{name}:\", test_metrics)\n",
    "\n",
    "    model.save_pretrained(run_name)\n",
    "    tok.save_pretrained(run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a949b52",
   "metadata": {},
   "source": [
    "## Train Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c5d69b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Prompt Tuning (10 tokens) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\3218247839.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 630,570 || all params: 67,616,340 || trainable%: 0.9326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.903300</td>\n",
       "      <td>2.856576</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.048310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tuning (10 tokens): {'eval_loss': 2.8726015090942383, 'eval_accuracy': 0.327, 'eval_f1_micro': 0.327, 'eval_f1_macro': 0.052532646030350225, 'eval_runtime': 4.9885, 'eval_samples_per_second': 400.923, 'eval_steps_per_second': 12.629, 'epoch': 1.0}\n",
      "\n",
      "=== Training Prompt Tuning (50 tokens) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\3218247839.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 661,290 || all params: 67,647,060 || trainable%: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 02:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.081700</td>\n",
       "      <td>3.038867</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.023913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tuning (50 tokens): {'eval_loss': 3.056018590927124, 'eval_accuracy': 0.2485, 'eval_f1_micro': 0.2485, 'eval_f1_macro': 0.02583327723837385, 'eval_runtime': 6.843, 'eval_samples_per_second': 292.271, 'eval_steps_per_second': 9.207, 'epoch': 1.0}\n",
      "\n",
      "=== Training Prompt Tuning (100 tokens) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_8140\\3218247839.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 699,690 || all params: 67,685,460 || trainable%: 1.0337\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 03:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.265200</td>\n",
       "      <td>3.234683</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.007230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tuning (100 tokens): {'eval_loss': 3.261652946472168, 'eval_accuracy': 0.1745, 'eval_f1_micro': 0.1745, 'eval_f1_macro': 0.00707494577226378, 'eval_runtime': 11.614, 'eval_samples_per_second': 172.206, 'eval_steps_per_second': 5.424, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from peft import PromptTuningConfig, get_peft_model\n",
    "\n",
    "# DistilBERT specifics\n",
    "num_layers = 6       # DistilBERT has 6 transformer layers\n",
    "token_dim = 768      # hidden size\n",
    "\n",
    "for num_virtual_tokens in [10, 50, 100]:\n",
    "    print(f\"\\n=== Training Prompt Tuning ({num_virtual_tokens} tokens) ===\")\n",
    "\n",
    "    # Base model\n",
    "    num_labels = len(label_names)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # ✅ Prompt tuning config (added num_layers + token_dim)\n",
    "    prompt_cfg = PromptTuningConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        num_virtual_tokens=num_virtual_tokens,\n",
    "        tokenizer_name_or_path=checkpoint,\n",
    "        num_layers=num_layers,    # must specify for DistilBERT\n",
    "        token_dim=token_dim,      # hidden size = 768 for DistilBERT-base\n",
    "        num_attention_heads=12,   # DistilBERT has 12 attention heads\n",
    "    )\n",
    "\n",
    "    # Wrap with PEFT\n",
    "    model = get_peft_model(model, prompt_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Training config\n",
    "    run_name = f\"huffpost-generic-distilbert-prompt-{num_virtual_tokens}\"\n",
    "    args = TrainingArguments(\n",
    "        output_dir=run_name,\n",
    "        run_name=run_name,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=200,\n",
    "        learning_rate=5e-5,             # slightly higher than LoRA/IA3\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator,\n",
    "        train_dataset=ds_tok[\"train\"],\n",
    "        eval_dataset=ds_tok[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    test_metrics = trainer.evaluate(ds_tok[\"test\"])\n",
    "    print(f\"Prompt Tuning ({num_virtual_tokens} tokens):\", test_metrics)\n",
    "\n",
    "    model.save_pretrained(run_name)\n",
    "    tok.save_pretrained(run_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
